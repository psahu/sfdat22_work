{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### INDEED #####\n",
    "\n",
    "# a job posting website (as well as resume hosting)\n",
    "# scrape some (or A LOT) of job postings from indeed for the job \"data scientist\"\n",
    "# Plan of Action:\n",
    "# 1. Figure out the url for getting the summaries (by doing it yourself!)\n",
    "# 2. Scrape the summary\n",
    "# 3. Figure out how to change the page by changing the URL (hint, click the next page button and see how the url changes)\n",
    "# 4. BONUS: count the most used words in the sumaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/jobs?q=Data Scientist&l=San Francisco, CA\n"
     ]
    }
   ],
   "source": [
    "# 1. Figure out the url for getting the summaries (by doing it yourself!)\n",
    "jobtitle = 'Data Scientist'\n",
    "location = 'San Francisco, CA'\n",
    "# http://www.indeed.com/jobs?q=Data+Scientist&l=San+Francisco%2C+CA # URL from web for reference\n",
    "url = 'http://www.indeed.com/jobs?q=' + jobtitle + '&l=' + location\n",
    "print(url)\n",
    "\n",
    "# Request from the web\n",
    "r = requests.get(url)\n",
    "b = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-5 years workforce experience in data visualization, reporting and analytics. This Consumer Data Visualization Analyst will work directly with Consumer Product...\n",
      "Strong analytical background, at least a BS, and preferably an MS or PhD, degree in computer science, statistics or related quantitative field, with an emphasis...\n",
      "When viewed as a sampler, the native operation of the D-Wave machine is analogous to a sparsely-connected restricted Boltzmann machine....\n",
      "\n",
      "We are creating the best way for people to shop for groceries by applying data, algorithms and machine learning to problems in logistics, retail,...\n",
      "\n",
      "Data Scientist, Expert*. Experience is collecting data from various data sources with different data platforms....\n",
      "\n",
      "Ability to provide statistical solutions and machine learning Algos for CE and qPCR instruments and solutions. BS in computer science....\n",
      "\n",
      "We are looking for an experienced machine learning engineer to join us. Whether that be a new technology you want to learn, an online class you want to take, or...\n",
      "\n",
      "Data Scientist, Expert– GRID (W2)*. Experience is collecting data from various data sources with different data platforms....\n",
      "\n",
      "Experience with SQL and Linux. Ideally experienced in working with large unstructured and structured data sets....\n",
      "\n",
      "Founding Data Scientist. Deep knowledge of machine learning, statistics, optimization or related field....\n",
      "\n",
      "Our data scientists work on every aspect of the product — whether it's exploratory research to understand user behavior for riders and drivers, or running...\n",
      "\n",
      "We are seeking a Data Scientist to lead the design, execution, and analysis of experiments in collaboration with our engineering team to improve the...\n",
      "\n",
      "Collaborate with other engineers, data scientists, and product/business stakeholders Qualifications. Conduct data mining to unearth new insights....\n",
      "Along with the analytics and fleet monitoring teams, you will work on performance monitoring and reporting tools, as well as implement machine learning...\n",
      "If you are passionate about digging into large data sets and extracting data-driven insights to address complex business problems through analysis and...\n"
     ]
    }
   ],
   "source": [
    "# 2. Scrape the summary\n",
    "summaries = b.findAll('span', attrs={'class':'summary'})\n",
    "for jobdesc in summaries:\n",
    "    print jobdesc.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Figure out how to change the page by changing the URL \n",
    "#(hint, click the next page button and see how the url changes)\n",
    "# http://www.indeed.com/jobs?q=Data+Scientist&l=San+Francisco%2C+CA&start=20 # url from web for reference\n",
    "\n",
    "def get_summaries(job, location, pgno):\n",
    "    limit = (pgno - 1)*10\n",
    "    url = 'http://www.indeed.com/jobs?q=' + jobtitle + '&l=' + location + '&start=' + str(limit)\n",
    "    #print url\n",
    "    r = requests.get(url)\n",
    "    b = BeautifulSoup(r.text, \"html.parser\")\n",
    "    summaries = b.findAll('span', attrs={'class':'summary'})\n",
    "    for jobdesc in summaries:\n",
    "        print jobdesc.text\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong analytical background, at least a BS, and preferably an MS or PhD, degree in computer science, statistics or related quantitative field, with an emphasis...\n",
      "If you are passionate about digging into large data sets and extracting data-driven insights to address complex business problems through analysis and...\n",
      "\n",
      "We're a nimble, sharp, and passionate team, and we're looking for data scientists who want to have impact on a real product used by real people and who have a...\n",
      "\n",
      "We are seeking a Data Scientist to lead the design, execution, and analysis of experiments in collaboration with our engineering team to improve the...\n",
      "\n",
      "Data Scientist, Expert*. Experience is collecting data from various data sources with different data platforms....\n",
      "\n",
      "Use your experience in analytics tools and scientific rigor to produce actionable insights. As a Data Scientist at Square you will lead projects that derive...\n",
      "\n",
      "Founding Data Scientist. Deep knowledge of machine learning, statistics, optimization or related field....\n",
      "\n",
      "Leveraging decades of innovative work in human-computer interaction, scalable data management, and machine learning, Trifacta’s unique Predictive Interaction...\n",
      "\n",
      "Ability to provide statistical solutions and machine learning Algos for CE and qPCR instruments and solutions. BS in computer science....\n",
      "\n",
      "Our team is made of software engineers, data scientists and financial industry veterans. Visualization, predictive analytics and personalization that enable...\n",
      "\n",
      "Have developed machine learning infrastructure. Seeking deep understanding. Have experience with Hadoop or other Big Data platforms....\n",
      "\n",
      "Experience with SQL and Linux. Ideally experienced in working with large unstructured and structured data sets....\n",
      "Senior Data Scientist with expertise in machine learning, artificial intelligence, statistical data analytics, and applications....\n",
      "Along with the analytics and fleet monitoring teams, you will work on performance monitoring and reporting tools, as well as implement machine learning...\n",
      "3-5 years workforce experience in data visualization, reporting and analytics. This Consumer Data Visualization Analyst will work directly with Consumer Product...\n"
     ]
    }
   ],
   "source": [
    "get_summaries('Data Scientist', 'San Francisco', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_summaries() takes exactly 3 arguments (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-abec15ee9978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msummarywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_summaries() takes exactly 3 arguments (0 given)"
     ]
    }
   ],
   "source": [
    "# 4. BONUS: count the most used words in the sumaries\n",
    "count = 0\n",
    "summarywords = {'data':0}\n",
    "for summary in get_summaries():\n",
    "    words = summary.split()\n",
    "    for word in words:\n",
    "        summarywords['data'] +=1\n",
    "        summarywords = summarywords.append()\n",
    "\n",
    "        \n",
    "summarywords_df = pd.DataFrame(summarywords, columns = ['word', 'count'])\n",
    "summarywords_df.sort_values(by='count', ascending= False).head(3)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### UFO ######\n",
    "\n",
    "# SCRAPE ALL SIGHTINGS in 2015\n",
    "# Plan of action\n",
    "# 1. Figure out the pattern to get a certain year and month from nuforc\n",
    "# 2. Scrape each sightings\n",
    "# 3. Make a dataframe out of it\n",
    "# 4. make a day column\n",
    "# 5. graph each day in 2015 and see which day had the most! (Is it July 4th?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### TWITTER ######\n",
    "\n",
    "# Create a dataframe where each row is a tweet that uses any tag you choose!\n",
    "# include a column for date, text, user handle, user name, and user image\n",
    "# You can use the normal query method and not stream for this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
